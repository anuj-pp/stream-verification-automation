# S3 Screenshot Debugger

A frontend-only web application for debugging ML pipeline accuracy by comparing ML API outputs, post-processing results, and database values against S3 screenshots.

üöÄ **[Live Demo - Deploy to GitHub Pages](https://pages.github.com)** (Instructions below)

## Purpose

This tool helps identify where issues occur in the game detection pipeline:
- **ML API**: Is the ML model detecting the wrong games?
- **ML Post-Processing**: Is the post-processing service filtering incorrectly?
- **Database**: Are the database values accurate?

By visualizing all three stages side-by-side with the actual screenshots, you can quickly identify where the airtime accuracy issues originate.

## Features

‚úÖ **No Backend Required** - Pure client-side application using AWS SDK for JavaScript  
‚úÖ **S3 Direct Access** - Fetch screenshots directly from S3 with temporary credentials  
‚úÖ **Side-by-Side Comparison** - View ML API, Post-Processing, and DB results together  
‚úÖ **Discrepancy Detection** - Automatic highlighting of mismatches between stages  
‚úÖ **Timeline Navigation** - Visual timeline showing all screenshots with discrepancy markers  
‚úÖ **Bounding Box Visualization** - Optional overlay of ML detection boxes on screenshots  
‚úÖ **Filtering** - Filter by discrepancy type to focus on specific issues  
‚úÖ **Export** - Export analysis results to CSV  
‚úÖ **Keyboard Shortcuts** - Fast navigation with arrow keys

## Deployment

### GitHub Pages Deployment

This application can be deployed to GitHub Pages for easy access from anywhere.

#### Option 1: Automatic Deployment (Recommended)

1. **Create a new GitHub repository** or use an existing one

2. **Initialize git and push** (if not already done):
   ```bash
   cd stream-verification-automation
   git init
   git add .
   git commit -m "Initial commit: S3 Screenshot Debugger"
   git branch -M main
   git remote add origin https://github.com/YOUR-USERNAME/YOUR-REPO.git
   git push -u origin main
   ```

3. **Enable GitHub Pages**:
   - Go to your repository on GitHub
   - Navigate to **Settings** ‚Üí **Pages**
   - Under "Build and deployment":
     - Source: **GitHub Actions**
   - The GitHub Actions workflow will automatically deploy on every push to `main`

4. **Access your app**:
   - Your app will be available at: `https://YOUR-USERNAME.github.io/YOUR-REPO/`
   - Wait 1-2 minutes for the first deployment

#### Option 2: Manual Deployment

If you prefer not to use GitHub Actions:

1. Go to **Settings** ‚Üí **Pages**
2. Under "Build and deployment":
   - Source: **Deploy from a branch**
   - Branch: **main** / **root**
3. Click **Save**
4. Access at: `https://YOUR-USERNAME.github.io/YOUR-REPO/`

#### Custom Domain (Optional)

To use a custom domain:
1. Add a `CNAME` file with your domain name
2. Configure DNS settings at your domain provider
3. Enable HTTPS in GitHub Pages settings

### Local Development

You can also run locally without deployment:

```bash
# Option 1: Direct file access
open index.html

# Option 2: Local server (recommended for testing)
python3 -m http.server 8080
# Then navigate to http://localhost:8080
```

## Setup

### 1. AWS Credentials

You'll need AWS credentials with read access to the S3 bucket containing screenshots:

- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_SESSION_TOKEN` (if using SSO/temporary credentials)

### 2. S3 Bucket Information

- **Bucket Name**: e.g., `streameranalytics-staging`
- **Region**: e.g., `eu-west-1`

### 3. Analysis JSON File

You need a `complete_analysis.json` file generated by the stream-verify tool. This file contains:
- Session metadata (platform, channel, date, session ID)
- Screenshot paths and timestamps
- ML API detection results
- Post-processing outputs
- Database session records
- Discrepancy flags

## Usage

### Step 1: Open the Application

Simply open `index.html` in your web browser. No server required!

```bash
# Option 1: Open directly
open index.html

# Option 2: Use a local server (optional)
python3 -m http.server 8080
# Then navigate to http://localhost:8080
```

### Step 2: Enter AWS Credentials

**Option A: Paste Export Format (Fastest)**

1. Click **"üí° Quick Paste - Export Format"** to expand
2. Paste your credentials in this format:
   ```bash
   export AWS_ACCESS_KEY_ID="YOUR_ACCESS_KEY_ID"
   export AWS_SECRET_ACCESS_KEY="YOUR_SECRET_ACCESS_KEY"
   export AWS_SESSION_TOKEN="YOUR_SESSION_TOKEN"
   ```
3. Click **"Parse and Fill Fields"**
4. The fields will auto-fill

**Option B: Manual Entry**

1. Fill in your AWS credentials in the form
2. Enter the S3 bucket name (e.g., `streameranalytics-staging`)
3. Enter the AWS region (e.g., `eu-west-1`)

**Then:**
4. Click **"Validate Credentials"**
5. Wait for validation success (‚úì Valid)

### Step 3: Upload Analysis JSON

1. Click **"Choose File"** under "Upload Analysis JSON"
2. Select your `complete_analysis.json` file
3. Click **"Load Analysis"**
4. Wait for the file to parse

### Step 4: Review Results

The interface will display:

**Session Information Panel**
- Platform, channel, date, session ID
- Total screenshots analyzed
- Number of discrepancies found

**Viewer Section**
- **Screenshot**: The actual image from S3 with optional bounding boxes
- **ML API Detection**: Raw ML model output (games, confidence, latency)
- **Post-Processing**: Filtered results after sliding window logic
- **Database Records**: Actual DB sessions with game names and airtime

**Discrepancy Alert** (if applicable)
- Highlights specific mismatches
- Explains the type of discrepancy
- Shows which games are affected

**Timeline**
- Visual representation of all screenshots
- Green = all stages match
- Red = has discrepancy
- Blue = current screenshot

### Step 5: Navigate

**Using Buttons:**
- Previous / Next
- First / Last
- Jump to specific index

**Using Keyboard:**
- `‚Üê` Previous screenshot
- `‚Üí` Next screenshot
- `Home` First screenshot
- `End` Last screenshot
- `B` Toggle bounding boxes

### Step 6: Filter (Optional)

Use filters to focus on specific issues:
- **Show only discrepancies** - Hide matching screenshots
- **ML vs Post-Processing** - Where ML detected but post-processing filtered
- **Post-Processing vs DB** - Where post-processing output doesn't match DB
- **Missing in DB** - Games that should be in DB but aren't
- **Extra in DB** - Games in DB that shouldn't be there

### Step 7: Export (Optional)

Click **"Export Results"** to download a CSV file with:
- Index, timestamp
- Game counts for each stage
- Discrepancy flags

## Understanding the Data

### ML API Detection

Shows raw output from the ML model:
- **Games**: List of detected games with IDs and confidence scores
- **Bounding Boxes**: Coordinates `[x1, y1, x2, y2]` of detected regions
- **Latency**: Time taken for ML inference
- **Uniform Frame**: Whether the frame is uniform (no content)

### Post-Processing

Shows filtered results after applying business logic:
- **Games**: Games that passed the sliding window threshold
- **Game Count**: Number of games after filtering
- **Event Type**: Type of event (GAMEPLAY, etc.)
- **Sliding Window State**: Current buffer contents
- **Applied Threshold**: Whether threshold logic was applied

### Database Records

Shows actual data stored in the database:
- **Game Name**: Human-readable name (e.g., "Stack Em", "Fruit Party")
- **Game Identifier**: Game ID (e.g., "hckgm_staem_b8f0")
- **Session ID**: Unique session identifier
- **True Airtime**: Duration in seconds (THIS IS THE KEY METRIC)
- **Start/End Times**: When the game session started and ended

## Discrepancy Types

### ML vs Post-Processing

**What it means:** ML detected games, but post-processing filtered them out

**Common causes:**
- Sliding window building up (first 5 frames) - **This is normal!**
- Confidence threshold not met
- Game disappeared too quickly

**How to diagnose:**
- Check sliding window state - if building up, this is expected
- Look at confidence scores - might be too low
- Check if game appears in subsequent frames

### Post-Processing vs DB

**What it means:** Post-processing created game sessions, but they're not in the database

**Common causes:**
- Database write failure
- Transaction rollback
- Network issue between services

**How to diagnose:**
- This is a **critical issue** - DB writes are failing
- Check post-processing service logs
- Verify database connectivity

### Missing in DB

**What it means:** Post-processing sent games, but they're not in the DB

**Similar to Post-Processing vs DB**, indicates write issues.

### Extra in DB

**What it means:** Database has sessions that weren't in post-processing output

**Common causes:**
- Session continued from previous screenshots
- Manual database edits
- Stale data from previous runs

## Troubleshooting

### Test Your Credentials First

Use the credential tester tool:
```bash
# Open in browser
open test-credentials.html

# Or with local server
python3 -m http.server 8080
# Then navigate to http://localhost:8080/test-credentials.html
```

This simple tool quickly tests if your credentials work before loading the full app.

### "Invalid credentials or bucket access denied"

**Common Issues:**

1. **Empty Session Token**: Leave blank if not using SSO (don't enter `""`)
2. **Wrong Region**: Verify bucket region matches
3. **Insufficient Permissions**: Need `s3:HeadBucket` and `s3:GetObject`
4. **Expired Token**: Refresh SSO credentials
5. **CORS Issues**: Use local server instead of opening file directly
   ```bash
   python3 -m http.server 8080
   ```

**See full troubleshooting guide:** [TROUBLESHOOTING.md](TROUBLESHOOTING.md)

### "Error loading screenshot"

- Screenshot may not exist in S3 at the specified path
- Credentials may have expired (refresh SSO token)
- Network connectivity issue

### "No data to display"

- Ensure the JSON file is valid `complete_analysis.json` format
- Check browser console for parsing errors
- Try with a smaller JSON file first

### Images not loading

- Check browser console for CORS errors
- Verify S3 bucket allows GetObject for your credentials
- Try refreshing your AWS session token

### Slow performance

- Large JSON files (1000+ screenshots) may take time to parse
- Images are cached, so navigation should speed up after first load
- Use filters to reduce the dataset

## File Structure

```
stream-verification-automation/
‚îú‚îÄ‚îÄ index.html              # Main application UI
‚îú‚îÄ‚îÄ README.md              # This file
‚îú‚îÄ‚îÄ .nojekyll              # Prevents GitHub Pages Jekyll processing
‚îú‚îÄ‚îÄ .gitignore             # Git ignore rules
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ deploy.yml     # GitHub Actions deployment workflow
‚îú‚îÄ‚îÄ css/
‚îÇ   ‚îî‚îÄ‚îÄ styles.css         # Dark theme styling
‚îî‚îÄ‚îÄ js/
    ‚îú‚îÄ‚îÄ aws-config.js      # AWS SDK configuration
    ‚îú‚îÄ‚îÄ s3-client.js       # S3 image fetching
    ‚îú‚îÄ‚îÄ json-parser.js     # Parse complete_analysis.json
    ‚îú‚îÄ‚îÄ discrepancy.js     # Discrepancy detection
    ‚îú‚îÄ‚îÄ comparison.js      # Side-by-side display
    ‚îú‚îÄ‚îÄ viewer.js          # Screenshot viewer & navigation
    ‚îî‚îÄ‚îÄ main.js            # Application initialization
```

## Security Notes

‚ö†Ô∏è **Important**: This tool runs entirely in your browser. Your AWS credentials are:
- Never sent to any external server (other than AWS)
- Not stored persistently (only bucket name and region are saved to localStorage)
- Used only to access S3

**Best practices:**
- Use temporary credentials (AWS SSO) when possible
- Don't share your credentials
- Clear browser cache after use on shared computers
- Use credentials with minimal permissions (read-only S3 access)

### GitHub Pages Security

Since this is deployed on GitHub Pages:
- **DO NOT** commit AWS credentials to the repository
- Credentials are entered by users at runtime only
- The application code is public (as it should be for client-side apps)
- All AWS access goes directly from browser to AWS (not through GitHub)

## Browser Compatibility

Tested on:
- ‚úÖ Chrome 90+
- ‚úÖ Firefox 88+
- ‚úÖ Safari 14+
- ‚úÖ Edge 90+

## Tips for Effective Debugging

1. **Start with the discrepancy filter** - Focus on problematic screenshots first
2. **Check sliding window state** - Many "discrepancies" are just the window building up
3. **Look for patterns** - If all screenshots have the same issue, it's likely systemic
4. **Compare timestamps** - Verify DB start/end times match screenshot timestamps
5. **Check game names** - Ensure ML game IDs map to correct game names in DB
6. **Monitor true_airtime** - This is the ultimate metric for accuracy
7. **Use keyboard shortcuts** - Much faster than clicking for navigation
8. **Export results** - Share CSV with team for collaborative debugging

## Example Workflow

**Scenario:** Product team reports that game airtime is inaccurate

1. Generate `complete_analysis.json` for the session in question
2. Open this tool and load the JSON
3. Enable "Show only discrepancies" filter
4. Navigate through discrepant screenshots
5. For each discrepancy:
   - Is ML detecting wrong? ‚Üí ML model issue
   - Is post-processing filtering incorrectly? ‚Üí Check sliding window logic
   - Is DB missing sessions? ‚Üí Database write issue
6. Document findings and export CSV for evidence
7. Fix the identified component
8. Re-run analysis to verify fix

## Support

For issues or questions:
1. Check browser console for errors
2. Verify JSON file structure matches expected format
3. Test with a small dataset first (20 screenshots)
4. Check AWS credentials and permissions

## License

Internal tool for debugging ML pipeline accuracy.
